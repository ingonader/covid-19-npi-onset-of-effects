## ######################################################################### ##
## function library
## ------------------------------------------------------------------------- ##
## for covid-19 analysis
## ######################################################################### ##

## ========================================================================= ##
## function definitions ####
## ========================================================================= ##

#' Creates a new instance of the TaskReg R6 class
#'
#' \code{blocked_TaskReg} Defines a new task to use in an mlr3 learners, and if given a blocking
#' variable, it will be added to the columns roles of the the task
#' 
#' @param dataset The dataset to define the task on (dataframe/tibble).
#' @param features The predictors (featrue names) of the model (vector of strings).
#' @param target_var The variable to use as the target of the model (string).
#' @param block_var The blocking variable to use when spliting the dataset (string). 
#' If NULL is given then no blocking will be defined.
#' 
#' @return A taskReg R6 class
#' 
#' 
blocked_TaskReg <- function(dataset, 
                            features,
                            target_var, 
                            block_var=NULL){
  
  # get the ML set
  mlset <- dataset[c(block_var, features, target_var)]
  task <- TaskRegr$new(id = target_var,
                       backend = mlset,
                       target = target_var
  )
  if(!is.null(block_var)){
    # add blocking variable
    task$col_roles$group <- block_var
    # remove the blocking variable from the features
    task$col_roles$feature <- features
  }
  return(task)
}
# --------------------------- #
# The out-of-the-box function that is used to extract train and test set 
# from a resampling object causes an error when the task is blocked by a gropping variable
# issue in github:
# https://github.com/mlr-org/mlr3/issues/518
#
# The following two functions are exactly the same as the original functions used in the resampling object
# with the exception that we force a cartesian join to avoid the error
custom_rsmp_train_set <- function(rsmp_obj, i){
  # get ids (groups)
  ids <- rsmp_obj$.__enclos_env__$private$.get_train(i)
  # if the task is not grouped then return the ids
  if (is.null(rsmp_obj$.__enclos_env__$private$.groups))
    return(ids)
  # else the ids will be group ids and we still need the actuall row ids
  else{
    # extract row ids
    train_set_ids <- rsmp_obj$.__enclos_env__$private$.groups[list(ids), on = "group", allow.cartesian=TRUE][[1L]]
    return(train_set_ids)}
}
custom_rsmp_test_set <- function(rsmp_obj, i){
  # get ids (groups)
  ids <- rsmp_obj$.__enclos_env__$private$.get_test(i)
  # if the task is not grouped then return the ids
  if (is.null(rsmp_obj$.__enclos_env__$private$.groups))
    return(ids)
  # else the ids will be group ids and we still need the actuall row ids
  else{
    # extract row ids
    test_set_ids <- rsmp_obj$.__enclos_env__$private$.groups[list(ids), on = "group", allow.cartesian=TRUE][[1L]]
    return(test_set_ids)}
}

# --------------------------- #
#' Extract the data splits from a resampling object
#' 
#' Given an mlr3 task, and a resampling object
#' \code{split_task_with_rsmp} extracts all train and test sets generated by the resampling object
#' 
#' Note that if you choose to pass an instatiated resampling object, it is important to use the same task 
#' for instantiating. In \code{split_task_with_rsmp} the resampling object will only be instantiated with 
#' the given task if it wasn't already.
#' 
#' @param task_obj An mlr3 task R6 class (e.g. The output of \link[mlr3]{TaskRegr$new})
#' @param rsmp_obj An mlr3 resampling R6 class (e.g. The output of \link[mlr3]{rsmp})
#' 
#' @return A named list of n+1 data.frames where n is the number of iterations/splits the resampling object generates
#' (e.g. number of folds in CV resampling), one element, \code{data_all}, will contain the full dataset, and each of 
#' the \code{1} to \code{n} elements will contain a train and a test set.
#' 
split_task_with_rsmp <- function(task_obj, rsmp_obj){
  # if the resampling object is not instantiated, then instantiate it with the given task
  if(!rsmp_obj$is_instantiated)
    rsmp_obj$instantiate(task_obj)
  all_data <- task_obj$backend$data(rows = 1:task_obj$nrow, 
                                    cols = task_obj$backend$colnames)
  ret_lst <- lapply(1:rsmp_obj$iters, 
                    function(itr){
                      train_row_ids <- custom_rsmp_train_set(rsmp_obj, itr)
                      test_row_ids <- custom_rsmp_test_set(rsmp_obj, itr)
                      
                      train_set <- as_tibble(all_data[list(train_row_ids), on='..row_id', allow.cartesian=TRUE])
                      #
                      test_set <- as_tibble(all_data[list(test_row_ids), on='..row_id', allow.cartesian=TRUE])
                      return(list('train' = train_set, 
                                  'test' = test_set))
                    })
  names(ret_lst) <- 1:rsmp_obj$iters
  ret_lst[['data_all']] <- all_data
  return(ret_lst)
}


#' Extract the data split for one iteration from a resampling object (bootstrap)
#' 
#' Given an mlr3 task, and a resampling object extracts a specific bootstrap split
#' generated by the resampling object
#' 
#' The resampling object needs to be instantiated. Note that if you choose to pass an 
#' instatiated resampling object, it is important to use the same task 
#' for instantiating. In \code{split_task_with_rsmp} the resampling object will only be 
#' instantiated with the given task if it wasn't already.
#' 
#' @param task_obj An mlr3 task R6 class (e.g. The output of \link[mlr3]{TaskRegr$new})
#' @param rsmp_obj An mlr3 resampling R6 class (e.g. The output of \link[mlr3]{rsmp})
#' @param itr The number of the iteration for that the data should be extracted
#' 
#' @return A named list of 2 data.frames \code{train}  and \code{test} containing the 
#' data of the iterations/split the resampling object generates
get_bootstrap_split <- function(task_obj, rsmp_obj, itr) {
  all_data <- task_obj$backend$data(rows = 1:task_obj$nrow, 
                                    cols = task_obj$backend$colnames)
  train_row_ids <- custom_rsmp_train_set(rsmp_obj, itr)
  test_row_ids <- custom_rsmp_test_set(rsmp_obj, itr)
  
  train_set <- as_tibble(all_data[list(train_row_ids), on='..row_id', allow.cartesian=TRUE])
  test_set <- as_tibble(all_data[list(test_row_ids), on='..row_id', allow.cartesian=TRUE])
  return(list('train' = train_set, 
              'test' = test_set))
}

# --------------------------- #
#' Get the effect of a feature on a model.
#' 
#' \code{generate_FeatEff} uses \link[iml]{Predictor$new} and \link[iml]{FeatureEffect$new} to extract 
#' the effect of the feature on the given model.
#' 
#' @param model Any machine learning model.
#' @param mod_features The features used in the model (vector of strings). 
#' @param mod_target The name of the target variable (string).
#' @param eff_dataset The data to be used to get the effect of the feature (data.frame).
#' @param eff_feature The name of feature used for the efect analysis (string).
#' @param eff_grid_size The size of the grid to be used in \link[iml]{FeatureEffect$new}
#' @param eff_method The method to use for computing the effect of the feature. Will be passed to
#' \link[iml]{FeatureEffect$new} and could be one of ('ale', 'pdp', 'ice', 'pdp + ice')
#' @param add_cols The names of additional columns in \code{eff_dataset} to have in the output (names as strings)
#' @param eff_grid_size_density A value between 0 and 1 defining the factor with which the grid size is
#'   calculated. One means that there will be a grid point for every value in the feature, 0.5 for every
#'   second, etc. Only will be applied if \code{eff_grid_size} is \code{NA}. Default is 1. 
#' @param eff_grid_size_min Minimum number of grid points. Only will be applied if 
#'   \code{eff_grid_size} is \code{NA}. Default is 50.
#' @param eff_grid_size_max Maximum number of grid points. Only will be applied if 
#'   \code{eff_grid_size} is \code{NA}. Default is 500.
#' @return A data.frame with the same number of rows as \code{eff_dataset}, and the following columns:
#' \itemize{
#'   \item \strong{add_cols:} The columns passed to \code{add_cols}.
#'   \item \strong{.x:} The values of the passed feature.
#'   \item \strong{.value:} The effect of the feature (the value returned by FeatEff).
#'   \item \strong{.type:} The method used to get the effect.
#'   \item \strong{measure_name:} The name of the feature.
#' }
#' 
generate_FeatEff <- function(model, mod_features, mod_target, 
                             eff_dataset, eff_feature, eff_grid_size, eff_method,
                             add_cols = NULL, eff_grid_size_density = 1, 
                             eff_grid_size_min = 50, eff_grid_size_max = 500){
  predictor <- Predictor$new(
    model = model,
    data = eff_dataset[mod_features],
    y = eff_dataset[mod_target]
  )
  ## calculate grid size, if not given:
  if (is.na(eff_grid_size)) {
    eff_grid_size <- round(diff(range(eff_dataset[[eff_feature]])) * eff_grid_size_density + 1)
    eff_grid_size <- min(eff_grid_size, eff_grid_size_max) # not larger than the given max
    eff_grid_size <- max(eff_grid_size, eff_grid_size_min) # not smaller than the given min
  }
  effs <- FeatureEffect$new(
    predictor = predictor, 
    feature = eff_feature, 
    grid.size = eff_grid_size,
    method = tolower(eff_method))
  
  effs_result <- effs$results
  # add results to dataset (instead of prediction - no need for the linear interpolation)
  eff_dataset <- eff_dataset %>% left_join(effs_result, by=eff_feature) 
  
  # check that the dataset has all values returned by FeatureEffect
  # return an error if not
  effect_values_in_dataset <- eff_dataset[c(eff_feature, '.value')] %>%
    filter(!is.na(.value)) %>% unique() %>% nrow()
  returned_effect_values <- effs$results %>% nrow()
  stopifnot(effect_values_in_dataset == returned_effect_values)
  
  # select the needed columns and standarise the names
  # (in case we need one dataframe with all features)
  eff_dataset <- eff_dataset[c(add_cols, eff_feature, '.value', '.type')]
  eff_dataset['measure_name'] <- eff_feature
  names(eff_dataset) <- names(eff_dataset) %>% gsub(eff_feature, '.x', .)
  return(eff_dataset)
}

# --------------------------- #
#' Train a model and get the effect of a feature
#'
#' \code{retrain_model} trains the given model with the given dataset
#' 
#' @param mod_dataset A dataframe.
#' @param mod_features The features used in the model (names as strings). 
#' @param mod_target The name of the target variable (string).
#' 
#' @return A copy of the model after retraining
#' 
retrain_model <- function(model, mod_dataset, mod_features, mod_target){
  model_copy <- model$clone(deep = TRUE)
  # define a task with the data
  train_task <- blocked_TaskReg(mod_dataset, 
                                mod_features, 
                                mod_target)
  
  # train the model with the given data
  model_copy$train(train_task)
  return(model_copy)
}

# --------------------- #
#' A wrapper function to get the feature effect per model.
#' 
#' The point of this \code{wrapper_train_generate_FeatEff} is to have a function that we can use
#' to loop through the number of iterations (or data splits) and extract the feature effect with each data split.
#'
#' @param itr The number of the data split (bootstrap iteration) used to train the model (int)
#' @param models_lst A list of trained models, each of which is the output of \code{retrain_model} (list)
#' @param ... Arguments to pass to \code{generate_FeatEff}
#'
#' @return A data.frame same as \code{generate_FeatEff} wiht the additional column \strong{itr_n}
#' which saves \code{itr} (i.e., the number of the split passed to the function) in the data.frame
wrapper_generate_FeatEff <- function(itr, models_lst, ...){
  model <- models_lst[[itr]]
  effect_res <- generate_FeatEff(model=model, ...)
  # add number of iteration to the result
  effect_res[['itr_n']] <- itr
  return(effect_res)
}

# ------------------ #
#' Extract the scores on the train and test sets from an AutoTuner.
#' 
#' @param itr_batch_comb a named vector of two integers specifying the batch number(batch_n)
#' and a cv iteration (cv_itr), which need to be scored
#' @param learner any mlr3 learner with a tuning instance (e.g., trained AutoTuner)
#' @param measures A list of measures to use for scoring the prediction, each measure sould be a 
#' an mlr3 measure R6 class (e.g. output of msr).
#'
#' @return data.frame with the scores for the train and test set used in the specifed iteration
#'
exctract_score_on_itr <- function(itr_batch_comb, learner, measures){
  cv_itr <- itr_batch_comb['cv_itr']
  batch_n <- itr_batch_comb['batch_n']
  pred <- learner$tuning_instance$archive()$resample_result[[batch_n]]$data$prediction[[cv_itr]]
  scores <- rbind(
    pred$train$score(measures) %>%
      t %>% as_tibble() %>% 
      mutate(cv_itr = cv_itr, batch_n = batch_n, set = 'train'),
    pred$test$score(measures) %>%
      t %>% as_tibble() %>% 
      mutate(cv_itr = cv_itr, batch_n = batch_n, set = 'test')
  )
  return(scores)
}

# ------------------ #
#' Plot the prediction and truth for data of a given group
#' 
#' @param group The group to filter for prediction (string).
#' @param dataset The dataset with the data to predict (data.frame).
#' @param group_col The column in the dataset which contains the groups (string).
#' @param learner The model (mlr3 learner).  
#' @param measures A list of measures to use for scoring the prediction, each measure sould be a 
#' an mlr3 measure R6 class (e.g. output of msr).
#' @param plot_title_postfix A string to append to the plots title.
#'
#' @return ggplot object
plot_pred_per_group <- function(group, dataset, group_col, learner, measures, plot_title_postfix=''){
  new_data <- dataset %>% filter(get(group_col) == group)
  
  task_new_data <- blocked_TaskReg(new_data, 
                                   varnames_features, 
                                   varnames_target, 
                                   varnames_block)
  pred <- learner$predict(task_new_data)
  scores <- pred$score(measures)
  p <- cbind(pred$data$tab, new_data) %>% 
    ggplot() + 
    geom_line(aes(date, response, color='Response')) + 
    geom_line(aes(date, truth, color='Truth')) +
    labs(title = paste(group, plot_title_postfix), 
         subtitle = paste(scores %>% names(), ':', round(scores, 3), 
                          collapse = '\n'),
         y=varnames_target) + 
    theme_minimal()
  return(p)
}

# ------------------ #
#' get performance measures
#' 
#' @param group The group to filter for prediction (string).
#' @param dataset The dataset with the data to predict (data.frame).
#' @param group_col The column in the dataset which contains the groups (string).
#' @param learner The model (mlr3 learner).  
#' @param measures A list of measures to use for scoring the prediction, each measure sould be a 
#'   an mlr3 measure R6 class (e.g. output of msr).
#' @param add_cor Add correlation to the list of measures (calculated from the predictions
#'   via the \code{cor()} function). Defaults to FALSE.
#' 
#' @return A named vector of the same length as measures containing the performance measures
get_measures_per_group <- function(group, dataset, group_col, learner, measures, add_cor = FALSE){
  new_data <- dataset %>% filter(get(group_col) == group)
  
  task_new_data <- blocked_TaskReg(new_data, 
                                   varnames_features, 
                                   varnames_target, 
                                   varnames_block)
  pred <- learner$predict(task_new_data)
  r <- cor(pred$truth, pred$response, use = "pair")
  scores <- pred$score(measures)
  if (add_cor)
    scores <- c(scores, "regr.r" = r)
  return(scores)
}
#get_measures_per_group("AUT", dat_all, "countrycode", learner, measures, add_cor = TRUE)


# ------------------ #
#' Plot the effect of a feature on a model.
#' 
#' @param feature The name of the feature (string).
#' @param effect_res The results of the feature effect analysis, specifically it should either be the output of
#' \code{wrapper_train_generate_FeatEff} or has the same columns (data.frame).
#' @param eff_method The method used to get the effect, for the title of the plot (string).
#' @param target The name of the target column for the title of the y-axis (string).
#' @param median_col The name of the column containing the median values. If NULL no median line will be added (string).
#' @param original_model The results of the feature effect analysis on the original model (before re-training).
#'  Sould have a similar structure to the output of \code{generate_FeatEff}. If NULL no line with the results 
#'  of the original model will be added (data.frame).
#' @param eff_dataset The data to be used to get the effect of the feature (data.frame). Used to calculate
#'   the rug, which represents how many countries we have data from in the dataset that we calculated
#'   the effect on. If NULL, then data data from one iteration is used.
#' @param addvertline Add vertical line at zero (TRUE/FALSE). Defaults to TRUE.
#' @param addtitle Add title to plot (TRUE/FALSE). Defaults to TRUE.
#' @param addcolorscale Add color scale (TRUE/FALSE). Defaults to TRUE.
#' @param clean_txt Should feature names be replaced with beautifed names? Defaults to FALSE.
#' @param addtheme Add theme_minimal()? (TRUE/FALSE). Defaults to TRUE.
#'  
#' @return ggplot object.
plot_feature_effect <- function(feature, effect_res, 
                                eff_method = '', target = '',
                                median_col = NULL, original_model = NULL, 
                                eff_dataset = NULL,
                                addvertline = TRUE,
                                addtitle = TRUE,
                                addcolorscale = TRUE,
                                clean_txt = FALSE,
                                addtheme = FALSE){
  # browser()
  plot_this <- effect_res %>%
    filter(measure_name == feature) 
  
  # line plot for each bootstrap sample (remove missing values for the line plot)
  p <- ggplot(data = plot_this %>%
                filter(!is.na(.value)),
              aes(.x, .value)) +
    geom_line(aes(group = itr_n, color = "Bootstrap"),
              alpha = 0.2, size = 0.2)
  
  # add vertial line at 0
  if (addvertline)
    p <- p + geom_vline(aes(xintercept=0),
                        linetype='dashed', alpha=0.3)
  
  if(!is.null(median_col)){
    # add the median line
    p <- p + geom_line(data = plot_this %>%
                         filter(!is.na(get(median_col))),
                       aes(y=get(median_col),
                           color='Median'),
                       size=1.3)
  }
  
  if(!is.null(original_model)){
    # add effect line on the original model
    p <- p +  geom_line(data = original_model %>%
                          filter(!is.na(.value)),
                        aes(color='Original model'),
                        size=0.8)
  }
  
  # add rug:
  if (!is.null(eff_dataset)) {
    # (use the data that is provided and use the number of countries):
    p <- p + geom_rug(data = eff_dataset,
                      sides = 'b',
                      alpha = 0.2,
                      position = 'jitter',
                      color="grey")
  } else {
    #(use the data from one iteration for the rug)
    p <- p + geom_rug(data = plot_this %>%
                        filter(itr_n==1),
                      sides = 'b',
                      alpha = 0.2,
                      position = 'jitter',
                      color="grey") 
    
  }
  # customize appearance
  x_lab <- get_unit(feature)
  # if(is.na(x_lab)) x_lab <- 'Days since first implemented'
  if (addcolorscale) {
    p <- p +
      scale_color_manual(
        '', 
        values = c('Median' = blue_itps_logo, 'Original model' = "darkgreen", "Bootstrap" = "grey"),
        labels = c('Median' = 'Median effect', 'Original model' = 'Complete training set', "Bootstrap" = "Bootstrap")
      )
    
  }
  if (addtitle) {
    ttl <- feature
    if (clean_txt) {
      ttl <- clean_txt(feature)
    }
    p <- p +     
      labs(title = ttl)
  }
  p <- p +
    labs(
         # subtitle = 'Effect plot with bootstrapping',
         x = x_lab,
         y = "Change in growth rate"   #paste0(eff_method, ' - ', target))
    )
  if (addtheme) {
    p <- p + theme_minimal()
  }
  p <- p +
    theme(plot.title = element_text(size = 25), 
          axis.title = element_text(size = 20),
          legend.text = element_text(size=20))
  return(p)
}


## Open data frame in WPS office:
Viewxl <- function(x, filename = NA) {
  ## define default filename if none is given:
  if (is.na(filename)) 
    filename <- paste0(
      "tmp-", 
      stringr::str_replace_all(Sys.time(), "[ :]", "-"), 
      ".xlsx"
    )
  ## write file if data is given:
  if (!is.null(x) && !is.na(x))
    writexl::write_xlsx(x, file.path(path_tmp, filename))
  
  ## open file:
  if (file.exists(file.path(path_tmp, filename)))
    system2("wps", args = file.path(path_tmp, filename), wait = FALSE)
  else
    warning("File ", file.path(path_tmp, filename), " not found.")
}

## Reorder Cormat 
## http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
reorder_cormat <- function(cormat){
  # Use correlation between variables as distance
  dd <- as.dist((1-cormat)/2)
  hc <- hclust(dd)
  cormat <-cormat[hc$order, hc$order]
}

## Get upper triangle of the correlation matrix
## http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

ggheatmap_cust <- function(dat, value) {
	p <- ggplot(dat, aes_string("Var2", "Var1", fill = value))+
    geom_tile(color = "white")+
    scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                         midpoint = 0, limit = c(-1,1), space = "Lab",
                         name="Pearson\nCorrelation") +
    theme_minimal()+ # minimal theme
    theme(axis.text.x = element_text(angle = 75, vjust = 1, hjust = 1, margin = margin(0, 0, 0, 0))) +
    coord_fixed() +
    #geom_text(aes(Var2, Var1, label = value), color = "black", size = 1.5, angle = 15) +
    theme(legend.position = c(-0.10, -0.15))

    return(p)
}

translate_measures <- function(mnames) {
  ret <- mnames %>% stringr::str_replace_all("_", " ") %>%
    gsub("(targeted|genpop)", "\\(\\1\\)", .) %>%
    stringr::str_replace_all("genpop", "gen. pop.") %>%
    stringr::str_to_sentence()
  return(ret)
}

theme_cust <- function() {
  theme_minimal()
}

format_float <- function(x, sprintfstring = "%.3f", comma_char = "Â·") {
  ret <- sprintf(sprintfstring, x) %>%
    stringr::str_replace_all("\\.", comma_char)
  return(ret)
}


get_mean_increase_ratio_unvec <- function(
  dat_j, 
  measure_name, 
  at = 0, 
  varname = "value_rel_smooth",
  conf_level = .95,
  retval = c("mean", "lwr.ci", "upr.ci", "n")
  ) 
{
  if (!(measure_name %in% names(dat_j))) {
    warning("Variable", measure_name, "not found in data.frame provided.")
    return(NA)
  }
  x <- dat_j[dat_j[[measure_name]] == 0, ][[varname]]
  ret <- DescTools::Gmean(x, conf.level = conf_level, na.rm = TRUE)
  ret <- c(ret, "n" = length(x))
  return(ret[retval])
}

get_mean_increase_ratio <- function(
  dat_j, 
  measure_name, 
  at = 0, 
  varname = "value_rel_smooth",
  conf_level = .95,
  retval = c("mean", "lwr.ci", "upr.ci", "n")
  )
{
  purrr::map_dbl(
    measure_name, 
    ~ get_mean_increase_ratio_unvec(
      dat_j, .x, at = at, varname = varname,
      conf_level = conf_level,
      retval = retval
      )
  )   
}
# get_mean_increase_ratio_unvec(dat_all, "visa_restrictions_targeted")
# get_mean_increase_ratio_unvec(dat_all, "visa_restrictions_targeted", retval = "mean")
# # DescTools::Gmean(c(1.1, 1.1, 1.2), conf.level = .95)


theme_itps <- function() {
  theme_minimal() + 
    theme(
      text = element_text(family = font_base)
    )
}

theme_npg <- function() {
    theme(
      panel.background = element_rect(fill = "white", color = "black"),
      legend.background = element_rect(fill = "white"),
      legend.key = element_rect(fill = "white")
      #axis.ticks.length = unit(-.25, "cm")
      #legend.box.background = element_rect(fill = "white")
      #axis.line = element_line()
    )
}

# ggplot(ToothGrowth, aes(x = factor(dose), y = len)) +
#   geom_boxplot() +
#   scale_y_continuous(position = "left", sec.axis = sec_axis(~., labels = NULL)) +
#   #scale_x_discrete(position = "bottom", sec.axis = sec_axis(~., labels = NULL)) +
#   theme_npg()

#theme_cust <- theme_itps

scale_x_continuous_npg <- function(...)
  scale_x_continuous(position = "bottom", sec.axis = dup_axis(name = NULL, labels = NULL), ...)
scale_y_continuous_npg <- function(...)
  scale_y_continuous(position = "left", sec.axis = dup_axis(name = NULL, labels = NULL), ...) #sec_axis(~., labels = NULL), ...)
# scale_y_discrete_npg <- function(...)
#   scale_y_discrete(position = "bottom", ...) # sec.axis = dup_axis(name = NULL, labels = NULL), ...)
scale_x_date_npg <- function(...)
  scale_x_date(position = "bottom", sec.axis = dup_axis(name = NULL, labels = NULL), ...)

clean_txt <- function(txt){
  txt_clean <- txt
  ## switch back short names to original names:
  for (i in 1:nrow(trans_measures_dat)) {
      txt_clean <- stringr::str_replace_all(txt_clean, 
        trans_measures_dat[["sanitized"]][i],
        trans_measures_dat[["original"]][i])
  }
  ## replace postfixes:
  txt_clean <- str_replace_all(txt_clean,
                               c('__subnat' = ' (subnat.',
                                 '__nat' = ' (nat.',
                                 '__man$' = '/man.)',
                                 '__vol$' = '/vol.)',
                                 '__miss$' = '/miss.)',
                                 '__man' = '/man.',
                                 '__vol' = '/vol.',
                                 '__miss' = '/miss.',
                                 '__in$' = '/inbound)',
                                 '__out$' = '/outbound)',
                                 '__inout$' = '/in- and outbound)',
                                 '__miss$' = '/miss.)'))
                                 # 'ext_' = 'External ',
                                 # 'int_' = 'Internal ',
  ## replace prefixes:
  txt_clean <- str_replace_all(txt_clean,
                               c('ctry_' = ''))
  ## replace other variable names:
  txt_clean <- str_replace_all(txt_clean,
                               c('pop65perc' = 'Population aged 65 and above', # (% of total population)',
                                 'popurbanperc' = 'Urban population', # (% of total population)',
                                 # 'popurban' = 'Urban population',
                                 'popdens' = 'Population density (people per sq. km of land area)',
                                 'poppollutionperc' = 'Population exposed to high levels of air pollution', # (% of total)',
                                 'gdppcppp_trans' = 'GDP (ppp) per capita (log-transformed)',
                                 'time_abs' = 'Days since COVID-19 was declared a pandemic',
                                 'time_rel' = 'Days since number of cases reached 25'))
  # txt_clean <- gsub('__', '/', txt_clean)
  # txt_clean <- gsub('_', ' ', txt_clean)
  # # enter a new line at the nearest '-' if text is too long
  # txt_clean <- gsub('(.{1,60})(-|$)', '\\1\n', txt_clean)
  #txt_clean <- str_to_title(txt_clean)
  return(txt_clean)
}
#clean_txt(varnames_features)

get_unit <- function(feature) {
  unt <- "[?]"
  if (feature %in% varnames_measures_all) 
    unt <- "Days since implementation"
  if (feature %in% c("time_rel", "time_abs"))
    unt <- "Days"
  if ( (feature %in% varnames_ctry) & grepl("perc$", feature))
    unt <- "Percent of total population"
  if (feature == "ctry_gdppcppp_trans")
    unt <- "GDP (ppp) per capita (log-transformed)"
  return(unt)
}
#purrr::map_chr(varnames_features, get_unit)


## --------------------------------------------------- ##
#' Produce list of feature effects plots (ALE plots)
#' 
#' Resulting list can be passed to arrange_and_save_ale_list(), potentially after
#' modifying the plots manually.
#' 
#' @param varnames_vec Vector of feature names that the ALE plots should be produced for
#' @return List of length 2: "plots" containing a named list of individual ggplot objects,
#'   "varnames_vec" the vector that was passed as a parameter.
produce_ale_list <- function(varnames_vec) {
  addvertline <- !grepl("^ctry_|^time_", varnames_vec[1])
  plots <- lapply(varnames_vec, 
                  function(feature){
                    tic(paste0('Time to create plot for ', feature))
                    # get the effect without the additional training
                    eff_noTraining <- generate_FeatEff(full_model, varnames_features, varnames_target, 
                                                       dat_all, feature, eff_grid_size, eff_method)
                    # create plot 
                    p <- plot_feature_effect(feature = feature, 
                                             effect_res = eff_all_feat_df,
                                             #effect_res = eff_all_feat_df %>% filter(itr_n <= 5),  ## [[DEVEL]] [[HERE]] !!! REMOVE !!!
                                             ## eff_method = eff_method, target = varnames_target, 
                                             median_col = 'value_median', original_model = eff_noTraining,
                                             eff_dataset = dat_all %>%
                                               select(one_of("countrycode", feature, "value_rel_smooth")) %>%
                                               setNames(c("countrycode", ".x", ".value")) %>%
                                               filter(.x > -15) %>%
                                               group_by(countrycode, .x) %>%
                                               summarize(
                                                 .value = first(.value), ## could be any, but dataset needs to have .value
                                                 .groups = "drop_last"
                                               ),
                                             addvertline = addvertline,
                                             addtitle = TRUE,
                                             addcolorscale = FALSE,
                                             clean_txt = TRUE,
                                             addtheme = FALSE)
                    toc()
                    return(p)
                  })
  ## modify all plots according to theme:
  plots <- plots %>% lapply(
    function(p)
      p + 
      theme_npg() +
      # scale_y_continuous(position = "left", sec.axis = sec_axis(~., labels = NULL)) + 
      # scale_x_continuous(position = "bottom", sec.axis = sec_axis(~., labels = NULL)) + 
      scale_color_manual(
        '',
        # values = c('Median' = ggsci::pal_npg()(10)[8],
        #            'Original model' = ggsci::pal_npg()(10)[5],
        #            'Bootstrap' = ggsci::pal_npg()(10)[6]),
        values = c('Median' = ggsci::pal_npg()(10)[9],
                   'Original model' = ggsci::pal_npg()(10)[10],
                   'Bootstrap' = "darkgrey"),
        labels = c('Median' = 'Median effect',
                   'Original model' = 'Complete training set',
                   'Bootstrap' = 'Boostrapped models')
      ) +
      #theme(plot.title = element_text(size = 22)) +
      theme(
        axis.text = element_text(size = 6),  ## axes tick marks
        plot.title = element_text(size = 10), 
        axis.title = element_text(size = 8),
        legend.text = element_text(size = 8)
      ) +
      theme(legend.position = "bottom")  ## legend will be extracted for ggarrange's common legend
  )
  names(plots) <- varnames_vec
  ## return list of plots and varnames_vec:
  return(list("plots" = plots, "varnames_vec" = varnames_vec))
}

## --------------------------------------------------- ##
#' Save panel plot
#' 
#' Takes an object produced by produce_ale_list(), which was potentially modified
#' before (axis ticks, etc.), and saves a panel figure to disk.
#' 
#' @param plotlist List procuded by produce_ale_list()
#' @param filename Filename for the resulting plot
#' @param ncol Number of columns in the panel plot
arrange_and_save_ale_list <- function(plotlist, filename, ncol = 2, ...) {
  ## get plotlist and varnames that were used to create the plots:
  plots <- plotlist[["plots"]]
  varnames_vec <- plotlist[["varnames_vec"]]
  plots_len <- length(plots)
  
  ## add to the list of plots other parameters to pass to ggarrange
  plots['ncol'] <- ncol
  plots['nrow'] <- ceiling(plots_len / plots[['ncol']])
  plots['common.legend'] <- TRUE
  plots['legend'] <- "bottom"
  
  ## extract legend from first plot object (needs to redraw for some reason):
  p_tmp <- plots[[1]]
  plot(p_tmp)
  legend_grob <- get_legend(p_tmp)
  plots[['legend.grob']] <- legend_grob
  
  ## call ggarrange with the list of parameters
  fig <- do.call(ggarrange, plots)
  
  ## save panel plot:
  ggsave(filename = filename,
         plot = fig,
         width = 5 * plots[["ncol"]],
         height = 3 * plots[["nrow"]],
         units = "in",
         dpi = 600,
         ...)
  
}
